# -*- coding: utf-8 -*-
"""

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OaXD1ZDFm0QCyKbEio6x47-haYOZ95tX

# Este Proyecto fue desarrollado en Google Colab

# Instalacion de Librerias
# """

# file_path = '/content/drive/MyDrive/requirements.txt'
# local_file_path = 'requirements.txt'

# try:
#     with open(file_path, 'r') as f:
#         libs = f.read()
#     with open(local_file_path, 'w') as f:
#         f.write(libs)
#     print(f"File content saved to {local_file_path}")
# except FileNotFoundError:
#     print(f"Error: File not found at {file_path}")
# except Exception as e:
#     print(f"An error occurred: {e}")

# !pip install -r requirements.txt

# !pip install -U langchain langchain-core langchain-text-splitters langchain-google-genai langchain-chroma langgraph

# !pip install "protobuf==3.20.3"

# !pip install notion-client

# !pip install --pre -U langchain langchain-openai

# !pip install langsmith

# !pip install -U langchain-tavily

"""Setup de la keys"""

from google.colab import userdata

#Modificar en caso de importar las claves en un .env, caso contrario añadir
#las claves a la seccion secretos con los nombres utilizados ejemplo: userdata.get('GOOGLE_API_KEY')

GEMINI_API_KEY = userdata.get('GOOGLE_API_KEY')
BD_ID = userdata.get('BD_ID')
NOTION_TOKEN = userdata.get('NOTION_TOKEN')
LANGCHAIN_API_KEY = userdata.get('LANGCHAIN_API_KEY')
LANGCHAIN_PROJECT = userdata.get('LANGCHAIN_PROJECT')
TAVILY_API_KEY = userdata.get('TAVILY_API_KEY')

"""Traqueo LangSmith"""

from langsmith import Client, traceable
import os
# 1. Habilita el "traceo" (seguimiento)
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"

# 2. Carga y valida la API key
if LANGCHAIN_API_KEY:
    os.environ["LANGCHAIN_API_KEY"] = LANGCHAIN_API_KEY
else:
    print("No se encontró 'LANGCHAIN_API_KEY' en Secrets. El traqueo no funcionará.")

# 3. Carga y valida el proyecto
if LANGCHAIN_PROJECT:
    os.environ["LANGCHAIN_PROJECT"] = LANGCHAIN_PROJECT
else:
    os.environ["LANGCHAIN_PROJECT"] = "default" # Usar "default" si no se define

"""Librerias"""

from typing import Sequence, Annotated, TypedDict, Literal

# Carga de variables de entorno
from dotenv import load_dotenv

# Componentes de LangChain
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.tools.retriever import create_retriever_tool
from langchain_core.tools import tool
from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, AIMessage

# Componentes específicos de Google
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI

from langchain_chroma import Chroma

# Componentes de LangGraph
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode

from IPython.display import Image, display


# Componentes de Talivy AI

from langchain_tavily import TavilySearch

"""Base de Conocimiento"""

def load_documents() -> list[Document]:
    """Carga los documentos que representan el catalogo y la información de la ferreteria."""


    # Un solo documento con todo el catalogo
    catalogo_ferreteria_text = """
      Herramientas Manuales:
      - Martillo de Uña Mango de Fibra: Martillo de acero forjado de alta resistencia, ideal para clavar y extraer clavos. Mango ergonómico antideslizante. Precio: $18. Especificaciones: Marca: Stanley, Modelo: STHT51391, Peso: 16 oz (450g).
      - Juego de Destornilladores (6 piezas): Incluye 3 destornilladores planos y 3 Phillips (estrella) con punta magnética para fácil agarre de tornillos. Precio: $25. Especificaciones: Marca: Bremen, Material: Acero Cromo-Vanadio, Incluye: Estuche organizador.

      Herramientas Eléctricas:
      - Taladro Percutor Inalámbrico 18V: Taladro potente con función de percusión para perforar en mampostería, madera y metal. 2 velocidades variables. Precio: $120. Especificaciones: Marca: DeWalt, Modelo: DCD776, Batería: 18V Litio (1.5 Ah), Incluye: 1 batería, 1 cargador, maletín.
      - Amoladora Angular (Esmeril) 4.5": Herramienta versátil para cortar, desbastar y lijar metal, cerámica o piedra. 11000 RPM. Precio: $75. Especificaciones: Marca: Bosch, Modelo: GWS 700, Potencia: 710W, Diámetro de disco: 4.5" (115mm).

      Pinturería:
      - Látex Interior Blanco Mate (4 Litros): Pintura al agua de alta cobertura, antihongo y bajo olor. Ideal para paredes y techos de interior. Precio: $35. Especificaciones: Marca: Sherwin Williams, Acabado: Mate, Rendimiento: 40 m² (aprox. por mano).
      - Kit Set de Pintor (3 piezas): Combo económico para trabajos de pintura. No gotea. Precio: $9. Especificaciones: Incluye: 1 Rodillo de lana 22cm, 1 Pincel N°10, 1 Bandeja de pintura.

      Tornillería y Fijaciones:
      - Caja Tornillos Autoperforantes (100u): Tornillos punta aguja para construcción en seco (Durlock/Pladur). Medida 6x1 (3.5x25mm). Precio: $5. Especificaciones: Tipo: T2 Aguja, Material: Acero fosfatado negro.
      - Tarugos Fischer SX (Pack 50u): Tarugos de nylon de expansión en 4 direcciones. Para ladrillo macizo y hueco. Precio: $7. Especificaciones: Marca: Fischer, Medida: N° 8 (para tornillo de 5mm).
        """
    menu_docs = [
        Document(
            page_content=catalogo_ferreteria_text,
            metadata={"source": "menu.txt"}
        )
    ]


    # Un solo documento con toda la información del negocio
    negocio_info = """
    La ferretería "El Tornillo Maestro" es un negocio familiar fundado por Ricardo Fernández, un técnico con más de 30 años de experiencia en el rubro de la construcción.
    Ubicación: Av. San Martín 567, Ciudad de Mendoza, Mendoza, Argentina.
    "El Tornillo Maestro" abre de lunes a viernes en horario corrido de 8 AM a 6 PM, y los sábados de 8 AM a 1 PM. Domingos cerrado.
    Teléfono (WhatsApp): +54 261 456-7890
    Email: ventas@eltornillomaestro.com.ar
    Especialidad: Herramientas eléctricas, tornillería a granel y materiales para construcción en seco (Durlock/Pladur).
    Servicios: Asesoramiento técnico personalizado para profesionales y proyectos hogareños.
    Stock: Contamos con un amplio depósito para abastecimiento inmediato.
    Metodos de Pago: Se aceptan pagos con tarjeta, Mercado Pago y se ofrecen cuentas corrientes a empresas.
    """
    info_docs = [
        Document(
            page_content=negocio_info,
            metadata={"source": "info.txt"}
        )
    ]

    return menu_docs + info_docs

# CREACIÓN DEL VECTORSTORE PERSISTENTE ---

def create_or_load_vectorstore(documents: list[Document], embedding_model) -> Chroma:
    """Divide los documentos y crea o carga una base de datos vectorial Chroma persistente."""
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    splits = text_splitter.split_documents(documents)

    vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_model)
    return vectorstore

"""# Herramientas

BD Vectorial
"""

#Definicion de la herramientas busqueda en la base de conocimiento y busqueda en internet mediante Tavily

def define_tools(vectorstore: Chroma) -> list:
    """Define las herramientas que el agente ferretero podrá utilizar."""
    retriever = vectorstore.as_retriever(search_kwargs={"k": 4}) # Aumentamos k para más contexto

    retriever_tool = create_retriever_tool(
        retriever,
        name="consultar_catalogo",
        description="Busca y recupera información sobre productos, stock, precios , cómo hacer un proyecto o info de la ferreteria El Tornillo Maestro, horario, ubicacion, servicios, metodos de pago, especialidad. "
    )

    search_tool = TavilySearch(
        name="buscar_en_internet",
        description="Busca en internet información general, noticias, clima o temas que no estén en el catálogo interno.",
        max_results=3,
        tavily_api_key=TAVILY_API_KEY
    )

    return [retriever_tool, search_tool]

"""Persistencia en Notion"""

import os
from typing import Sequence, Annotated, TypedDict, Literal
from datetime import datetime
from langgraph.prebuilt import ToolNode
from pydantic.v1 import BaseModel, Field
from notion_client import Client

# El LLM leerá estas descripciones para saber qué enviar
class ReporteFerreteriaInput(BaseModel):
    resumen: str = Field(description="Título o resumen corto del reporte de ventas. Ej: 'Reporte de ventas 19/10/2025'")
    total_ventas: float = Field(description="Monto total de las ventas del día en números.")
    detalle: str = Field(description="Un detalle completo o resumen en texto de los eventos más importantes del día, productos más vendidos, etc.")

@tool(args_schema=ReporteFerreteriaInput)
def guardar_reporte_en_notion(resumen: str, total_ventas: float, detalle: str) -> str:
    """
    Guarda un reporte de ventas estructurado en la base de datos de Notion
    'Reportes de Ventas' de la ferretería.
    """
    try:
        notion = Client(auth=NOTION_TOKEN)
        fecha_hoy = datetime.now().strftime("%Y-%m-%d")

        # Crear la nueva página (el nuevo registro) en la base de datos
        notion.pages.create(
            parent={"database_id": BD_ID},
            properties={
                "Resumen": {
                    "title": [{"text": {"content": resumen}}]
                },
                "Fecha": {
                    "date": {"start": fecha_hoy}
                },
                "Total Ventas": {
                    "number": total_ventas
                },
                "Detalle": {
                    "rich_text": [{"text": {"content": detalle}}]
                }
            }
        )
        return f"Reporte '{resumen}' guardado exitosamente en Notion."
    except Exception as e:
        return f"Error al guardar en Notion: {e}"

"""Grafo"""

class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]

# Lista de mensajes internos que no queremos que los LLM vean
INTERNAL_MESSAGES = ["informant", "researcher", "__end__"]

def filter_internal_messages(messages: Sequence[BaseMessage]) -> list[BaseMessage]:
    """
    Filtra los mensajes de ruteo interno del supervisor.
    Esta versión maneja de forma segura 'msg.content' que no son strings.
    """
    filtered_list = []
    for msg in messages:
        # Primero, comprobamos si es un AIMessage Y si su contenido es un string
        if isinstance(msg, AIMessage) and isinstance(msg.content, str):
            # Si es un string, lo limpiamos y comprobamos si es un mensaje interno
            if msg.content.strip().lower() in INTERNAL_MESSAGES:
                continue # Es un mensaje interno, lo saltamos

        # Si no es un AIMessage,
        # o si su contenido no es un string (es una lista, None, etc.),
        # o si no es un mensaje interno,
        # lo añadimos a la lista.
        filtered_list.append(msg)

    return filtered_list

from langchain_core.messages import AIMessage # Asegúrate de que AIMessage esté importado al inicio

def supervisor_node(state: AgentState, llm):
    """
    Invoca al LLM con el rol supervisor.
    Esta versión es 'stateful' y optimizada:
    1. Si un trabajador acaba de terminar, termina el ciclo SIN llamar al LLM.
    2. Si el usuario habla, llama al LLM para enrutar.
    """
    members = ["researcher", "informant"]

    # Primero, comprobamos de quién es el último mensaje.

    if not isinstance(state["messages"][-1], HumanMessage):
        # Si el último mensaje NO es del usuario (es un AIMessage o ToolMessage),
        # significa que un trabajador (researcher o informant) acaba de terminar.
        # El ciclo de trabajo está completo. No necesitamos llamar al LLM.
        # Simplemente devolvemos el mensaje __end__ para terminar el grafo.
        return {"messages": [AIMessage(content="__end__")]}



    # Ahora procedemos a llamar al LLM para que decida el enrutamiento.

    system_prompt = f"""

    Eres un supervisor en una ferretería. Tu trabajo es enrutar la conversación
    al trabajador correcto, basándote en el *historial completo* de la conversación.

    Opciones válidas: {', '.join(members)} o "__end__".

    1.  **Continuar Tarea (lo más importante):**
        - Si el historial reciente trata sobre un **reporte de ventas** (ej. el cliente está dando
          datos como un total, un detalle, o la palabra "confirmar"),
          DEBES elegir: "informant".
        - Si el historial reciente trata sobre **productos o info del local** (ej. el cliente
          está respondiendo a Pedro o pidiendo más detalles), DEBES elegir: "researcher".

    2.  **Nueva Tarea:**
        - Si el último mensaje es una **pregunta nueva** sobre "generar un reporte"
          o algo administrativo, elige: "informant".
        - Si el último mensaje es una **pregunta nueva** sobre productos, stock, precios,
          o info del local, elige: "researcher".

    3.  **Terminar:**
        - Si la conversación parece terminada (ej. "gracias", "adiós")
          o no puedes clasificarla, elige: "__end__".

    Responde SOLAMENTE con una de las opciones válidas (ej. "researcher", "informant", o "__end__").
    """

    # Filtramos los mensajes de ruteo ANTES de enviarlos al LLM
    filtered_messages = filter_internal_messages(state["messages"])
    messages = [SystemMessage(content=system_prompt)] + filtered_messages

    response = llm.invoke(messages)

    # Limpieza de la respuesta del LLM
    clean_response = response.content.strip().replace("'", "").replace("\"", "")

    if "__end__" in clean_response:
         response.content = "__end__"
    elif "informant" in clean_response:
        response.content = "informant"
    elif "researcher" in clean_response:
        response.content = "researcher"
    else:
        # Fallback por si el LLM no responde bien
        response.content = "__end__"

    return {"messages": [response]}

def researcher_node(state: AgentState, llm_with_tools):
    """Nodo de investigación: maneja consultas de productos, precios o stock."""
    system_prompt = """
    Eres "Pedro", un empleado de la ferretería "El Tornillo Maestro".
    Eres amable, servicial y eficiente.

    Tu misión:
    - Responder preguntas sobre productos, precios, categorías, materiales y stock.
    - Usa SIEMPRE la herramienta `consultar_catalogo` para cualquier pregunta relacionada con productos, precios, categorías, materiales y stock.
    - Si el cliente menciona una categoría (por ejemplo: "pinturería", "herramientas", "electricidad", etc.),
    usa la herramienta para buscar productos en esa categoría.
    - Si el cliente pide "listar todos los productos", usa la herramienta y mostra todas las categorias y sus elementos.
    - Si te preguntan por datos del negocio o la ferreteria:
    nombre de la ferretería, ubicacion,
    especialidad (los tipos de herramientas que venden electricas, a granel, construccion),
    servicios(solo responde acerca de los servicios profesionales y hogareños), horario, telefono,
    stock (deposito almacenamiento inmediato), email o metodos de pago (tarjeta , mercado pago, cuenta corriente),
    solo responde tarjeta , mercado pago, cuenta corriente cuando se piden metodos de pago
    utiliza la herramienta  `consultar_catalogo`' para consultar dicha informacion.
    - Si la pregunta no está relacionada con la ferretería o cualquier cosa que NO esté en el catálogo (ej. "cómo arreglar un grifo que gotea"
      o "precio del cemento hoy a nivel nacional"), DEBES usar la herramienta `buscar_en_internet`.

    Formato de respuesta:
    - No expliques lo que haces ni menciones herramientas.
    - No digas “voy a buscar” o “puedo ofrecerte”.
    - Da SOLO la respuesta final (por ejemplo, una lista o breve descripción con precios).
    """

    filtered_messages = filter_internal_messages(state["messages"])
    messages = [SystemMessage(content=system_prompt)] + filtered_messages

    response = llm_with_tools.invoke(messages)
    return {"messages": [response]}

def informant_node(state: AgentState, llm_with_notion):
    """
    Nodo para generar y guardar reportes de ventas en Notion.
    Esta versión sigue un proceso estricto de 4 pasos.
    """

    # Este prompt es estricto para evitar que el LLM
    # alucine datos y para forzar el paso de "confirmación".
    system_prompt = """
    Eres el analista de negocios de la ferretería 'El Tornillo Maestro'.
    Tu objetivo es crear reportes de ventas y guardarlos en Notion.
    SIGUE ESTE PROCESO ESTRICTAMENTE:

    PASO 1: PEDIR DATOS
    - Si el historial de conversación NO contiene un resumen, un total de ventas Y un detalle,
      tu ÚNICA respuesta debe ser pedirle al usuario los tres datos.
    - Tu respuesta debe ser: "Claro, estoy listo para crear el reporte. Por favor, indícame el resumen, el total de ventas y el detalle."
    - NO INVENTES NINGÚN DATO. NUNCA. NO ASUMAS NADA.

    PASO 2: PEDIR CONFIRMACIÓN
    - Si el historial SÍ contiene los datos (resumen, total, detalle) proporcionados por el usuario,
      pero el último mensaje del usuario NO es "confirmar",
      tu ÚNICA respuesta debe ser pedirle al usuario que confirme.
    - Tu respuesta debe ser: "Gracias, he recibido los datos. Por favor, escribe 'confirmar' para guardar el reporte en Notion."

    PASO 3: GUARDAR (Tool Call)
    - Si el historial contiene los datos Y el último mensaje del usuario SÍ es "confirmar",
      DEBES llamar a la herramienta `guardar_reporte_en_notion`.
    - Debes extraer el resumen, total_ventas y detalle del historial de la conversación
      para pasarlos como argumentos a la herramienta.

    PASO 4: CONFIRMAR GUARDADO
    - Si el historial contiene un `ToolMessage` (que es el resultado de la herramienta),
      tu ÚNICA respuesta debe ser informar al usuario del resultado.
    - Ejemplo de respuesta: "¡Reporte guardado exitosamente en Notion!" o "Error: No se pudo guardar el reporte."

    CONSIDERACIONES FINALES
    - No respondas al usuario con el JSON a guardar, mostralo como items y su valor.
    - No respondas al usuario la enumeracion de pasos Ej "PASO 1".

    """


    filtered_messages = filter_internal_messages(state["messages"])
    messages = [SystemMessage(content=system_prompt)] + filtered_messages

    response = llm_with_notion.invoke(messages)

    return {"messages": [response]}

def should_continue(state: AgentState) -> Literal["researcher", "informant", "__end__"]:
    """
    Lee el último mensaje (del supervisor) y decide el siguiente paso.
    Comprueba si la decisión es 'researcher', 'informant', o 'END'.
    """

    last_message = state["messages"][-1]

    #El "supervisor_node" fue un LLM instruido para responder con "researcher", "informant", o "END".

    # Buscamos la decisión en el contenido del mensaje
    content = last_message.content.lower()

    if "researcher" in content:
        return "researcher"
    elif "informant" in content:
        return "informant"
    else:
        # Si no es ninguno de los trabajadores, terminamos el grafo.
        return "__end__"

def route_researcher(state: AgentState) -> Literal["tools", "__end__"]:
    """
    Decide si el investigador necesita usar herramientas o si ha terminado.
    Si ha terminado, la ruta "__end__" lo devolverá al supervisor
    (basado en cómo lo conectaremos en el grafo).
    """
    last_message = state["messages"][-1]
    if last_message.tool_calls:
        # Si el LLM pidió una herramienta, vamos al nodo "tools"
        return "tools"

    # Si no pidió herramienta (es una respuesta final), volvemos al supervisor
    return "__end__"


def route_informant(state: AgentState) -> Literal["notion", "__end__"]:
    last_message = state["messages"][-1]


    if last_message.tool_calls:
        return "notion"

    return "__end__"

@traceable(run_type="chain", name="Construir grafo")
def build_graph(llm,llm_with_tools, tools_list,llm_with_notion,notion_tool):
    """Construye y compila el grafo del agente mozo."""

    graph = StateGraph(AgentState)

    graph.add_node("supervisor", lambda state: supervisor_node(state,llm))

    graph.add_node("researcher", lambda state: researcher_node(state,llm_with_tools)) # Pass llm_with_tools to researcher

    graph.add_node("informant", lambda state: informant_node(state,llm_with_notion))

    graph.add_node("tools", ToolNode(tools_list))

    graph.add_node("notion", ToolNode(notion_tool))

    graph.set_entry_point("supervisor")
    graph.add_conditional_edges(
        "supervisor", should_continue, {"researcher": "researcher", "informant": "informant" ,"__end__": END}
    )

    graph.add_conditional_edges(
        "researcher",
        route_researcher, # Lee la respuesta del investigador
        {
            "tools": "tools",       # Si hay tool_calls, va a "tools"
            "__end__": "supervisor" # Si no hay tools, vuelve al "supervisor"
        }
    )

    graph.add_conditional_edges(
        "informant",
        route_informant, # Esta lee la respuesta del investigador
        {
            "notion": "notion",       # Si hay tool_calls, va a "tools"
            "__end__": "supervisor" # Si no hay tools, vuelve al "supervisor"
        }
    )

    graph.add_edge("tools","researcher")
    graph.add_edge("notion","informant")

    return graph.compile()

"""Ejecucion Principal"""

llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash",google_api_key=GEMINI_API_KEY, temperature=0)
embedding_model = GoogleGenerativeAIEmbeddings(model="models/gemini-embedding-001",google_api_key=GEMINI_API_KEY)
documents = load_documents()
vectorstore = create_or_load_vectorstore(documents, embedding_model)
tools1 = define_tools(vectorstore)
notion_tool = [guardar_reporte_en_notion]
llm_with_tools1 = llm.bind_tools(tools1)
llm_with_notion = llm.bind_tools(notion_tool)

rag_agent = build_graph(llm,llm_with_tools1,tools1,llm_with_notion,notion_tool)

def extract_text(msg):
     """Devuelve texto legible de un mensaje, sin importar su formato."""
     content = getattr(msg, "content", "")
     if not content:
         return ""
     if isinstance(content, list):
         return " ".join(
             part.get("text", "") if isinstance(part, dict) else str(part)
             for part in content
         ).strip()
     elif isinstance(content, str):
         return content.strip()
     else:
         return str(content).strip()
conversation_history = []
print("\n🤖 Ferretería El Tornillo Maestro: ¡Hola! Soy tu asistente. ¿En qué puedo ayudarte?")
print("   Puedes preguntar por productos (ej. 'martillos') o 'crear reporte'. Escribe 'salir' para terminar.")

while True:
    query = input("\n👤 Cliente: ")
    if query.lower() in ["exit", "quit", "salir"]:
        print("\n👋 Ferretería El Tornillo Maestro: ¡Gracias por tu visita! 🧰")
        break

    # Agregar el mensaje del usuario
    conversation_history.append(HumanMessage(content=query))

    # Invocar al agente RAG
    result = rag_agent.invoke({"messages": conversation_history})

    # Actualizar historial con la respuesta
    conversation_history = result["messages"]

    # Buscar el último mensaje con texto útil
    final_response = None
    for msg in reversed(conversation_history):
        text = extract_text(msg)

        # Limpiamos cualquier token de ruteo
        clean_text = text.replace("__end__", "").replace("informant", "").replace("researcher", "").strip(" :")


        if clean_text: # Si queda texto, es la respuesta
            final_response = clean_text
            break

    if not final_response:
        final_response = "Lo siento, tuve un problema al procesar eso. ¿Puedes intentarlo de nuevo?"

    print(f"\n🤖 Ferretería El Tornillo Maestro: {final_response}")

"""Mostramos el Grafo"""

from IPython.display import Image, display
display(Image(rag_agent.get_graph().draw_mermaid_png()))


png_data = rag_agent.get_graph().draw_mermaid_png()

# 2. Define un nombre para el archivo
filename = "mi_agente_grafo.png"

# 3. Escribe esos bytes en un archivo
# (usamos "wb" que significa "write binary")
with open(filename, "wb") as f:
    f.write(png_data)