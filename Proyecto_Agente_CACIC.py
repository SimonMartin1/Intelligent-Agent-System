# -*- coding: utf-8 -*-
"""

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OaXD1ZDFm0QCyKbEio6x47-haYOZ95tX

# Este Proyecto fue desarrollado en Google Colab

# Instalacion de Librerias
# """

# file_path = '/content/drive/MyDrive/requirements.txt'
# local_file_path = 'requirements.txt'

# try:
#     with open(file_path, 'r') as f:
#         libs = f.read()
#     with open(local_file_path, 'w') as f:
#         f.write(libs)
#     print(f"File content saved to {local_file_path}")
# except FileNotFoundError:
#     print(f"Error: File not found at {file_path}")
# except Exception as e:
#     print(f"An error occurred: {e}")

# !pip install -r requirements.txt

# !pip install -U langchain langchain-core langchain-text-splitters langchain-google-genai langchain-chroma langgraph

# !pip install "protobuf==3.20.3"

# !pip install notion-client

# !pip install --pre -U langchain langchain-openai

# !pip install langsmith

# !pip install -U langchain-tavily

"""Setup de la keys"""

from google.colab import userdata

#Modificar en caso de importar las claves en un .env, caso contrario aÃ±adir
#las claves a la seccion secretos con los nombres utilizados ejemplo: userdata.get('GOOGLE_API_KEY')

GEMINI_API_KEY = userdata.get('GOOGLE_API_KEY')
BD_ID = userdata.get('BD_ID')
NOTION_TOKEN = userdata.get('NOTION_TOKEN')
LANGCHAIN_API_KEY = userdata.get('LANGCHAIN_API_KEY')
LANGCHAIN_PROJECT = userdata.get('LANGCHAIN_PROJECT')
TAVILY_API_KEY = userdata.get('TAVILY_API_KEY')

"""Traqueo LangSmith"""

from langsmith import Client, traceable
import os
# 1. Habilita el "traceo" (seguimiento)
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"

# 2. Carga y valida la API key
if LANGCHAIN_API_KEY:
    os.environ["LANGCHAIN_API_KEY"] = LANGCHAIN_API_KEY
else:
    print("No se encontrÃ³ 'LANGCHAIN_API_KEY' en Secrets. El traqueo no funcionarÃ¡.")

# 3. Carga y valida el proyecto
if LANGCHAIN_PROJECT:
    os.environ["LANGCHAIN_PROJECT"] = LANGCHAIN_PROJECT
else:
    os.environ["LANGCHAIN_PROJECT"] = "default" # Usar "default" si no se define

"""Librerias"""

from typing import Sequence, Annotated, TypedDict, Literal

# Carga de variables de entorno
from dotenv import load_dotenv

# Componentes de LangChain
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.tools.retriever import create_retriever_tool
from langchain_core.tools import tool
from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, AIMessage

# Componentes especÃ­ficos de Google
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI

from langchain_chroma import Chroma

# Componentes de LangGraph
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode

from IPython.display import Image, display


# Componentes de Talivy AI

from langchain_tavily import TavilySearch

"""Base de Conocimiento"""

def load_documents() -> list[Document]:
    """Carga los documentos que representan el catalogo y la informaciÃ³n de la ferreteria."""


    # Un solo documento con todo el catalogo
    catalogo_ferreteria_text = """
      Herramientas Manuales:
      - Martillo de UÃ±a Mango de Fibra: Martillo de acero forjado de alta resistencia, ideal para clavar y extraer clavos. Mango ergonÃ³mico antideslizante. Precio: $18. Especificaciones: Marca: Stanley, Modelo: STHT51391, Peso: 16 oz (450g).
      - Juego de Destornilladores (6 piezas): Incluye 3 destornilladores planos y 3 Phillips (estrella) con punta magnÃ©tica para fÃ¡cil agarre de tornillos. Precio: $25. Especificaciones: Marca: Bremen, Material: Acero Cromo-Vanadio, Incluye: Estuche organizador.

      Herramientas ElÃ©ctricas:
      - Taladro Percutor InalÃ¡mbrico 18V: Taladro potente con funciÃ³n de percusiÃ³n para perforar en mamposterÃ­a, madera y metal. 2 velocidades variables. Precio: $120. Especificaciones: Marca: DeWalt, Modelo: DCD776, BaterÃ­a: 18V Litio (1.5 Ah), Incluye: 1 baterÃ­a, 1 cargador, maletÃ­n.
      - Amoladora Angular (Esmeril) 4.5": Herramienta versÃ¡til para cortar, desbastar y lijar metal, cerÃ¡mica o piedra. 11000 RPM. Precio: $75. Especificaciones: Marca: Bosch, Modelo: GWS 700, Potencia: 710W, DiÃ¡metro de disco: 4.5" (115mm).

      PinturerÃ­a:
      - LÃ¡tex Interior Blanco Mate (4 Litros): Pintura al agua de alta cobertura, antihongo y bajo olor. Ideal para paredes y techos de interior. Precio: $35. Especificaciones: Marca: Sherwin Williams, Acabado: Mate, Rendimiento: 40 mÂ² (aprox. por mano).
      - Kit Set de Pintor (3 piezas): Combo econÃ³mico para trabajos de pintura. No gotea. Precio: $9. Especificaciones: Incluye: 1 Rodillo de lana 22cm, 1 Pincel NÂ°10, 1 Bandeja de pintura.

      TornillerÃ­a y Fijaciones:
      - Caja Tornillos Autoperforantes (100u): Tornillos punta aguja para construcciÃ³n en seco (Durlock/Pladur). Medida 6x1 (3.5x25mm). Precio: $5. Especificaciones: Tipo: T2 Aguja, Material: Acero fosfatado negro.
      - Tarugos Fischer SX (Pack 50u): Tarugos de nylon de expansiÃ³n en 4 direcciones. Para ladrillo macizo y hueco. Precio: $7. Especificaciones: Marca: Fischer, Medida: NÂ° 8 (para tornillo de 5mm).
        """
    menu_docs = [
        Document(
            page_content=catalogo_ferreteria_text,
            metadata={"source": "menu.txt"}
        )
    ]


    # Un solo documento con toda la informaciÃ³n del negocio
    negocio_info = """
    La ferreterÃ­a "El Tornillo Maestro" es un negocio familiar fundado por Ricardo FernÃ¡ndez, un tÃ©cnico con mÃ¡s de 30 aÃ±os de experiencia en el rubro de la construcciÃ³n.
    UbicaciÃ³n: Av. San MartÃ­n 567, Ciudad de Mendoza, Mendoza, Argentina.
    "El Tornillo Maestro" abre de lunes a viernes en horario corrido de 8 AM a 6 PM, y los sÃ¡bados de 8 AM a 1 PM. Domingos cerrado.
    TelÃ©fono (WhatsApp): +54 261 456-7890
    Email: ventas@eltornillomaestro.com.ar
    Especialidad: Herramientas elÃ©ctricas, tornillerÃ­a a granel y materiales para construcciÃ³n en seco (Durlock/Pladur).
    Servicios: Asesoramiento tÃ©cnico personalizado para profesionales y proyectos hogareÃ±os.
    Stock: Contamos con un amplio depÃ³sito para abastecimiento inmediato.
    Metodos de Pago: Se aceptan pagos con tarjeta, Mercado Pago y se ofrecen cuentas corrientes a empresas.
    """
    info_docs = [
        Document(
            page_content=negocio_info,
            metadata={"source": "info.txt"}
        )
    ]

    return menu_docs + info_docs

# CREACIÃ“N DEL VECTORSTORE PERSISTENTE ---

def create_or_load_vectorstore(documents: list[Document], embedding_model) -> Chroma:
    """Divide los documentos y crea o carga una base de datos vectorial Chroma persistente."""
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    splits = text_splitter.split_documents(documents)

    vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_model)
    return vectorstore

"""# Herramientas

BD Vectorial
"""

#Definicion de la herramientas busqueda en la base de conocimiento y busqueda en internet mediante Tavily

def define_tools(vectorstore: Chroma) -> list:
    """Define las herramientas que el agente ferretero podrÃ¡ utilizar."""
    retriever = vectorstore.as_retriever(search_kwargs={"k": 4}) # Aumentamos k para mÃ¡s contexto

    retriever_tool = create_retriever_tool(
        retriever,
        name="consultar_catalogo",
        description="Busca y recupera informaciÃ³n sobre productos, stock, precios , cÃ³mo hacer un proyecto o info de la ferreteria El Tornillo Maestro, horario, ubicacion, servicios, metodos de pago, especialidad. "
    )

    search_tool = TavilySearch(
        name="buscar_en_internet",
        description="Busca en internet informaciÃ³n general, noticias, clima o temas que no estÃ©n en el catÃ¡logo interno.",
        max_results=3,
        tavily_api_key=TAVILY_API_KEY
    )

    return [retriever_tool, search_tool]

"""Persistencia en Notion"""

import os
from typing import Sequence, Annotated, TypedDict, Literal
from datetime import datetime
from langgraph.prebuilt import ToolNode
from pydantic.v1 import BaseModel, Field
from notion_client import Client

# El LLM leerÃ¡ estas descripciones para saber quÃ© enviar
class ReporteFerreteriaInput(BaseModel):
    resumen: str = Field(description="TÃ­tulo o resumen corto del reporte de ventas. Ej: 'Reporte de ventas 19/10/2025'")
    total_ventas: float = Field(description="Monto total de las ventas del dÃ­a en nÃºmeros.")
    detalle: str = Field(description="Un detalle completo o resumen en texto de los eventos mÃ¡s importantes del dÃ­a, productos mÃ¡s vendidos, etc.")

@tool(args_schema=ReporteFerreteriaInput)
def guardar_reporte_en_notion(resumen: str, total_ventas: float, detalle: str) -> str:
    """
    Guarda un reporte de ventas estructurado en la base de datos de Notion
    'Reportes de Ventas' de la ferreterÃ­a.
    """
    try:
        notion = Client(auth=NOTION_TOKEN)
        fecha_hoy = datetime.now().strftime("%Y-%m-%d")

        # Crear la nueva pÃ¡gina (el nuevo registro) en la base de datos
        notion.pages.create(
            parent={"database_id": BD_ID},
            properties={
                "Resumen": {
                    "title": [{"text": {"content": resumen}}]
                },
                "Fecha": {
                    "date": {"start": fecha_hoy}
                },
                "Total Ventas": {
                    "number": total_ventas
                },
                "Detalle": {
                    "rich_text": [{"text": {"content": detalle}}]
                }
            }
        )
        return f"Reporte '{resumen}' guardado exitosamente en Notion."
    except Exception as e:
        return f"Error al guardar en Notion: {e}"

"""Grafo"""

class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]

# Lista de mensajes internos que no queremos que los LLM vean
INTERNAL_MESSAGES = ["informant", "researcher", "__end__"]

def filter_internal_messages(messages: Sequence[BaseMessage]) -> list[BaseMessage]:
    """
    Filtra los mensajes de ruteo interno del supervisor.
    Esta versiÃ³n maneja de forma segura 'msg.content' que no son strings.
    """
    filtered_list = []
    for msg in messages:
        # Primero, comprobamos si es un AIMessage Y si su contenido es un string
        if isinstance(msg, AIMessage) and isinstance(msg.content, str):
            # Si es un string, lo limpiamos y comprobamos si es un mensaje interno
            if msg.content.strip().lower() in INTERNAL_MESSAGES:
                continue # Es un mensaje interno, lo saltamos

        # Si no es un AIMessage,
        # o si su contenido no es un string (es una lista, None, etc.),
        # o si no es un mensaje interno,
        # lo aÃ±adimos a la lista.
        filtered_list.append(msg)

    return filtered_list

from langchain_core.messages import AIMessage # AsegÃºrate de que AIMessage estÃ© importado al inicio

def supervisor_node(state: AgentState, llm):
    """
    Invoca al LLM con el rol supervisor.
    Esta versiÃ³n es 'stateful' y optimizada:
    1. Si un trabajador acaba de terminar, termina el ciclo SIN llamar al LLM.
    2. Si el usuario habla, llama al LLM para enrutar.
    """
    members = ["researcher", "informant"]

    # Primero, comprobamos de quiÃ©n es el Ãºltimo mensaje.

    if not isinstance(state["messages"][-1], HumanMessage):
        # Si el Ãºltimo mensaje NO es del usuario (es un AIMessage o ToolMessage),
        # significa que un trabajador (researcher o informant) acaba de terminar.
        # El ciclo de trabajo estÃ¡ completo. No necesitamos llamar al LLM.
        # Simplemente devolvemos el mensaje __end__ para terminar el grafo.
        return {"messages": [AIMessage(content="__end__")]}



    # Ahora procedemos a llamar al LLM para que decida el enrutamiento.

    system_prompt = f"""

    Eres un supervisor en una ferreterÃ­a. Tu trabajo es enrutar la conversaciÃ³n
    al trabajador correcto, basÃ¡ndote en el *historial completo* de la conversaciÃ³n.

    Opciones vÃ¡lidas: {', '.join(members)} o "__end__".

    1.  **Continuar Tarea (lo mÃ¡s importante):**
        - Si el historial reciente trata sobre un **reporte de ventas** (ej. el cliente estÃ¡ dando
          datos como un total, un detalle, o la palabra "confirmar"),
          DEBES elegir: "informant".
        - Si el historial reciente trata sobre **productos o info del local** (ej. el cliente
          estÃ¡ respondiendo a Pedro o pidiendo mÃ¡s detalles), DEBES elegir: "researcher".

    2.  **Nueva Tarea:**
        - Si el Ãºltimo mensaje es una **pregunta nueva** sobre "generar un reporte"
          o algo administrativo, elige: "informant".
        - Si el Ãºltimo mensaje es una **pregunta nueva** sobre productos, stock, precios,
          o info del local, elige: "researcher".

    3.  **Terminar:**
        - Si la conversaciÃ³n parece terminada (ej. "gracias", "adiÃ³s")
          o no puedes clasificarla, elige: "__end__".

    Responde SOLAMENTE con una de las opciones vÃ¡lidas (ej. "researcher", "informant", o "__end__").
    """

    # Filtramos los mensajes de ruteo ANTES de enviarlos al LLM
    filtered_messages = filter_internal_messages(state["messages"])
    messages = [SystemMessage(content=system_prompt)] + filtered_messages

    response = llm.invoke(messages)

    # Limpieza de la respuesta del LLM
    clean_response = response.content.strip().replace("'", "").replace("\"", "")

    if "__end__" in clean_response:
         response.content = "__end__"
    elif "informant" in clean_response:
        response.content = "informant"
    elif "researcher" in clean_response:
        response.content = "researcher"
    else:
        # Fallback por si el LLM no responde bien
        response.content = "__end__"

    return {"messages": [response]}

def researcher_node(state: AgentState, llm_with_tools):
    """Nodo de investigaciÃ³n: maneja consultas de productos, precios o stock."""
    system_prompt = """
    Eres "Pedro", un empleado de la ferreterÃ­a "El Tornillo Maestro".
    Eres amable, servicial y eficiente.

    Tu misiÃ³n:
    - Responder preguntas sobre productos, precios, categorÃ­as, materiales y stock.
    - Usa SIEMPRE la herramienta `consultar_catalogo` para cualquier pregunta relacionada con productos, precios, categorÃ­as, materiales y stock.
    - Si el cliente menciona una categorÃ­a (por ejemplo: "pinturerÃ­a", "herramientas", "electricidad", etc.),
    usa la herramienta para buscar productos en esa categorÃ­a.
    - Si el cliente pide "listar todos los productos", usa la herramienta y mostra todas las categorias y sus elementos.
    - Si te preguntan por datos del negocio o la ferreteria:
    nombre de la ferreterÃ­a, ubicacion,
    especialidad (los tipos de herramientas que venden electricas, a granel, construccion),
    servicios(solo responde acerca de los servicios profesionales y hogareÃ±os), horario, telefono,
    stock (deposito almacenamiento inmediato), email o metodos de pago (tarjeta , mercado pago, cuenta corriente),
    solo responde tarjeta , mercado pago, cuenta corriente cuando se piden metodos de pago
    utiliza la herramienta  `consultar_catalogo`' para consultar dicha informacion.
    - Si la pregunta no estÃ¡ relacionada con la ferreterÃ­a o cualquier cosa que NO estÃ© en el catÃ¡logo (ej. "cÃ³mo arreglar un grifo que gotea"
      o "precio del cemento hoy a nivel nacional"), DEBES usar la herramienta `buscar_en_internet`.

    Formato de respuesta:
    - No expliques lo que haces ni menciones herramientas.
    - No digas â€œvoy a buscarâ€ o â€œpuedo ofrecerteâ€.
    - Da SOLO la respuesta final (por ejemplo, una lista o breve descripciÃ³n con precios).
    """

    filtered_messages = filter_internal_messages(state["messages"])
    messages = [SystemMessage(content=system_prompt)] + filtered_messages

    response = llm_with_tools.invoke(messages)
    return {"messages": [response]}

def informant_node(state: AgentState, llm_with_notion):
    """
    Nodo para generar y guardar reportes de ventas en Notion.
    Esta versiÃ³n sigue un proceso estricto de 4 pasos.
    """

    # Este prompt es estricto para evitar que el LLM
    # alucine datos y para forzar el paso de "confirmaciÃ³n".
    system_prompt = """
    Eres el analista de negocios de la ferreterÃ­a 'El Tornillo Maestro'.
    Tu objetivo es crear reportes de ventas y guardarlos en Notion.
    SIGUE ESTE PROCESO ESTRICTAMENTE:

    PASO 1: PEDIR DATOS
    - Si el historial de conversaciÃ³n NO contiene un resumen, un total de ventas Y un detalle,
      tu ÃšNICA respuesta debe ser pedirle al usuario los tres datos.
    - Tu respuesta debe ser: "Claro, estoy listo para crear el reporte. Por favor, indÃ­came el resumen, el total de ventas y el detalle."
    - NO INVENTES NINGÃšN DATO. NUNCA. NO ASUMAS NADA.

    PASO 2: PEDIR CONFIRMACIÃ“N
    - Si el historial SÃ contiene los datos (resumen, total, detalle) proporcionados por el usuario,
      pero el Ãºltimo mensaje del usuario NO es "confirmar",
      tu ÃšNICA respuesta debe ser pedirle al usuario que confirme.
    - Tu respuesta debe ser: "Gracias, he recibido los datos. Por favor, escribe 'confirmar' para guardar el reporte en Notion."

    PASO 3: GUARDAR (Tool Call)
    - Si el historial contiene los datos Y el Ãºltimo mensaje del usuario SÃ es "confirmar",
      DEBES llamar a la herramienta `guardar_reporte_en_notion`.
    - Debes extraer el resumen, total_ventas y detalle del historial de la conversaciÃ³n
      para pasarlos como argumentos a la herramienta.

    PASO 4: CONFIRMAR GUARDADO
    - Si el historial contiene un `ToolMessage` (que es el resultado de la herramienta),
      tu ÃšNICA respuesta debe ser informar al usuario del resultado.
    - Ejemplo de respuesta: "Â¡Reporte guardado exitosamente en Notion!" o "Error: No se pudo guardar el reporte."

    CONSIDERACIONES FINALES
    - No respondas al usuario con el JSON a guardar, mostralo como items y su valor.
    - No respondas al usuario la enumeracion de pasos Ej "PASO 1".

    """


    filtered_messages = filter_internal_messages(state["messages"])
    messages = [SystemMessage(content=system_prompt)] + filtered_messages

    response = llm_with_notion.invoke(messages)

    return {"messages": [response]}

def should_continue(state: AgentState) -> Literal["researcher", "informant", "__end__"]:
    """
    Lee el Ãºltimo mensaje (del supervisor) y decide el siguiente paso.
    Comprueba si la decisiÃ³n es 'researcher', 'informant', o 'END'.
    """

    last_message = state["messages"][-1]

    #El "supervisor_node" fue un LLM instruido para responder con "researcher", "informant", o "END".

    # Buscamos la decisiÃ³n en el contenido del mensaje
    content = last_message.content.lower()

    if "researcher" in content:
        return "researcher"
    elif "informant" in content:
        return "informant"
    else:
        # Si no es ninguno de los trabajadores, terminamos el grafo.
        return "__end__"

def route_researcher(state: AgentState) -> Literal["tools", "__end__"]:
    """
    Decide si el investigador necesita usar herramientas o si ha terminado.
    Si ha terminado, la ruta "__end__" lo devolverÃ¡ al supervisor
    (basado en cÃ³mo lo conectaremos en el grafo).
    """
    last_message = state["messages"][-1]
    if last_message.tool_calls:
        # Si el LLM pidiÃ³ una herramienta, vamos al nodo "tools"
        return "tools"

    # Si no pidiÃ³ herramienta (es una respuesta final), volvemos al supervisor
    return "__end__"


def route_informant(state: AgentState) -> Literal["notion", "__end__"]:
    last_message = state["messages"][-1]


    if last_message.tool_calls:
        return "notion"

    return "__end__"

@traceable(run_type="chain", name="Construir grafo")
def build_graph(llm,llm_with_tools, tools_list,llm_with_notion,notion_tool):
    """Construye y compila el grafo del agente mozo."""

    graph = StateGraph(AgentState)

    graph.add_node("supervisor", lambda state: supervisor_node(state,llm))

    graph.add_node("researcher", lambda state: researcher_node(state,llm_with_tools)) # Pass llm_with_tools to researcher

    graph.add_node("informant", lambda state: informant_node(state,llm_with_notion))

    graph.add_node("tools", ToolNode(tools_list))

    graph.add_node("notion", ToolNode(notion_tool))

    graph.set_entry_point("supervisor")
    graph.add_conditional_edges(
        "supervisor", should_continue, {"researcher": "researcher", "informant": "informant" ,"__end__": END}
    )

    graph.add_conditional_edges(
        "researcher",
        route_researcher, # Lee la respuesta del investigador
        {
            "tools": "tools",       # Si hay tool_calls, va a "tools"
            "__end__": "supervisor" # Si no hay tools, vuelve al "supervisor"
        }
    )

    graph.add_conditional_edges(
        "informant",
        route_informant, # Esta lee la respuesta del investigador
        {
            "notion": "notion",       # Si hay tool_calls, va a "tools"
            "__end__": "supervisor" # Si no hay tools, vuelve al "supervisor"
        }
    )

    graph.add_edge("tools","researcher")
    graph.add_edge("notion","informant")

    return graph.compile()

"""Ejecucion Principal"""

llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash",google_api_key=GEMINI_API_KEY, temperature=0)
embedding_model = GoogleGenerativeAIEmbeddings(model="models/gemini-embedding-001",google_api_key=GEMINI_API_KEY)
documents = load_documents()
vectorstore = create_or_load_vectorstore(documents, embedding_model)
tools1 = define_tools(vectorstore)
notion_tool = [guardar_reporte_en_notion]
llm_with_tools1 = llm.bind_tools(tools1)
llm_with_notion = llm.bind_tools(notion_tool)

rag_agent = build_graph(llm,llm_with_tools1,tools1,llm_with_notion,notion_tool)

def extract_text(msg):
     """Devuelve texto legible de un mensaje, sin importar su formato."""
     content = getattr(msg, "content", "")
     if not content:
         return ""
     if isinstance(content, list):
         return " ".join(
             part.get("text", "") if isinstance(part, dict) else str(part)
             for part in content
         ).strip()
     elif isinstance(content, str):
         return content.strip()
     else:
         return str(content).strip()
conversation_history = []
print("\nðŸ¤– FerreterÃ­a El Tornillo Maestro: Â¡Hola! Soy tu asistente. Â¿En quÃ© puedo ayudarte?")
print("   Puedes preguntar por productos (ej. 'martillos') o 'crear reporte'. Escribe 'salir' para terminar.")

while True:
    query = input("\nðŸ‘¤ Cliente: ")
    if query.lower() in ["exit", "quit", "salir"]:
        print("\nðŸ‘‹ FerreterÃ­a El Tornillo Maestro: Â¡Gracias por tu visita! ðŸ§°")
        break

    # Agregar el mensaje del usuario
    conversation_history.append(HumanMessage(content=query))

    # Invocar al agente RAG
    result = rag_agent.invoke({"messages": conversation_history})

    # Actualizar historial con la respuesta
    conversation_history = result["messages"]

    # Buscar el Ãºltimo mensaje con texto Ãºtil
    final_response = None
    for msg in reversed(conversation_history):
        text = extract_text(msg)

        # Limpiamos cualquier token de ruteo
        clean_text = text.replace("__end__", "").replace("informant", "").replace("researcher", "").strip(" :")


        if clean_text: # Si queda texto, es la respuesta
            final_response = clean_text
            break

    if not final_response:
        final_response = "Lo siento, tuve un problema al procesar eso. Â¿Puedes intentarlo de nuevo?"

    print(f"\nðŸ¤– FerreterÃ­a El Tornillo Maestro: {final_response}")

"""Mostramos el Grafo"""

from IPython.display import Image, display
display(Image(rag_agent.get_graph().draw_mermaid_png()))


png_data = rag_agent.get_graph().draw_mermaid_png()

# 2. Define un nombre para el archivo
filename = "mi_agente_grafo.png"

# 3. Escribe esos bytes en un archivo
# (usamos "wb" que significa "write binary")
with open(filename, "wb") as f:
    f.write(png_data)